

import os
from dotenv import load_dotenv
from huggingface_hub import login
from ctransformers import AutoModelForCausalLM

# -----------------------------
# 1) Load HF token from .env
# -----------------------------
load_dotenv()
HF_TOKEN = os.getenv("HF_TOKEN")

if HF_TOKEN is None:
    raise ValueError("⚠️ Hugging Face token not found — please add HF_TOKEN to .env file.")

login(token=HF_TOKEN)

# -----------------------------
# 2) Load the Llama 2 model
# -----------------------------
model_id = "TheBloke/Llama-2-7B-chat-GGML"

config = {
    "max_new_tokens": 80,   #طول-150
    "repetition_penalty": 1.1,
    "temperature": 0.7,      # غيرته من 0.8 عشان يكون أقل عشوائية
    "stream": False,
}

print("⏳ Loading LLM — please wait...")
llm = AutoModelForCausalLM.from_pretrained(
    model_id,
    model_type="llama",
    gpu_layers=0,  #نشغّله على  CPU
    **config
)
print("✅ LLM loaded successfully!")

# -----------------------------
# 3) Chat-style prompt
# -----------------------------

SYSTEM_PROMPT = (
    "You are TrustLens, a helpful and concise AI assistant. "
    "Answer as a friendly chat partner. "
    "Stay on the same topic as the user's last message, "
    "and answer in the same language as the user. "
    "Keep the answer short (1–4 sentences) unless the user asks for more detail."
)

def _build_prompt(user_message: str) -> str:
    """
    نبني برومبت بالشكل المتوقع من Llama 2 Chat
    باستخدام فورمات [INST] ... [/INST] مع system prompt.
    """
    return (
        f"[INST] <<SYS>>\n{SYSTEM_PROMPT}\n<</SYS>>\n\n"
        f"{user_message.strip()}\n[/INST]"
    )

# -----------------------------
# 4) Public API for Flask
# -----------------------------
PROMPT_TEMPLATE = """
You are a friendly and concise AI assistant participating in a short conversation task for a platform called TrustLens.

Guidelines:
- Answer in 2 to 4 sentences maximum.
- Stay on the same topic started by the user.
- Be neutral and avoid harmful, offensive, or political content.
- If the user writes in Arabic, answer in Arabic. Otherwise answer in English.

User: {user_message}
Assistant:
"""

def generate_reply(user_message: str) -> str:
    """
    Takes a message written by an examiner and returns
    a response generated by the LLM model.
    """
    
    try:
        prompt = PROMPT_TEMPLATE.format(user_message=user_message)
        response = llm(prompt, stream=False)
        return str(response).strip()
    except Exception as e:
        print("⚠️ LLM error:", e)
        return "⚠️ Error processing the message, please try again."
