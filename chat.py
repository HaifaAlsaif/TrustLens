# chat.py â€” Ù…Ø­Ø§Ø¯Ø«Ø© Ø¨Ø³ÙŠØ·Ø© Ø¨Ø§Ù„ØªØ±Ù…ÙŠÙ†Ù„ Ù…Ø¹ Ù†Ù…ÙˆØ°Ø¬ LLM7B Ø®Ø§Øµ Ø¹Ø¨Ø± Hugging Face + ctransformers
from dotenv import load_dotenv
import os
from huggingface_hub import hf_hub_download
from ctransformers import AutoModelForCausalLM

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙˆÙƒÙ† Ù…Ù† Ù…Ù„Ù .env
load_dotenv()
HF_TOKEN = os.getenv("HUGGINGFACE_TOKEN") or os.getenv("HF_TOKEN")

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
MODEL_REPO = "TheBloke/Llama-2-7B-chat-GGML"  # Ø§Ø³ØªØ¨Ø¯Ù„ÙŠÙ‡Ø§ Ø¥Ø°Ø§ Ø¹Ù†Ø¯Ùƒ Ø±ÙŠØ¨ÙˆØ²ÙŠØªÙˆØ±ÙŠ Ø¢Ø®Ø±
MODEL_FILE = "llama-2-7b-chat.ggmlv3.q4_K_M.bin"  # Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù Ø¯Ø§Ø®Ù„ Ø§Ù„Ø±ÙŠØ¨ÙˆØ²ÙŠØªÙˆØ±ÙŠ (ØªÙ‚Ø¯Ø±ÙŠÙ† ØªØªØ£ÙƒØ¯ÙŠÙ† Ù…Ù†Ù‡ ÙÙŠ Ø§Ù„ØµÙØ­Ø©)
CONFIG = {
    "model_type": "llama",
    "max_new_tokens": 128,
    "temperature": 0.8,
    "repetition_penalty": 1.1,
    "stream": False,
    "gpu_layers": 0
}

print("â³ Downloading model from Hugging Face (with token)...")

# Ù†Ø­Ù…Ù„ Ø§Ù„Ù…Ù„Ù Ø£ÙˆÙ„ Ù…Ø±Ø© Ø¨Ø§Ù„ØªÙˆÙƒÙ† ÙˆÙ†Ø®Ø²Ù†Ù‡ Ù…Ø­Ù„ÙŠØ§Ù‹
model_path = hf_hub_download(
    repo_id=MODEL_REPO,
    filename=MODEL_FILE,
    token=HF_TOKEN
)

print("âœ… Download complete. Loading model...")

# Ù†Ø­Ù…Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø­Ù„ÙŠ
llm = AutoModelForCausalLM.from_pretrained(model_path, **CONFIG)

print("âœ… Model loaded successfully!\nğŸ¤– Terminal Chat â€” Ø§ÙƒØªØ¨ exit Ù„Ù„Ø®Ø±ÙˆØ¬.\n")

# Ø­Ù„Ù‚Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
history = []
while True:
    user = input("You: ").strip()
    if not user:
        continue
    if user.lower() in {"exit", "quit"}:
        print("Bye!")
        break

    prompt = (
        "You are a helpful assistant. Keep answers short and clear.\n\n"
        + "\n".join(history[-4:])
        + f"\nUser: {user}\nAssistant:"
    )
    reply = llm(prompt, stream=False)
    print(f"Assistant: {reply}\n")

    history += [f"User: {user}", f"Assistant: {reply}"]
